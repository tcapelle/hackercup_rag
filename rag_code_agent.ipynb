{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcapelle/hackercup_rag/blob/main/rag_code_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{rag-hackercup} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "# W&B Lighting Competition - AI Hacker Cup \n",
    "\n",
    "</a>\n",
    "\n",
    "[Weights & Biases](https://wandb.ai/site?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) are running a 7-day Lightning Competition focussed on solving practice problems for the  [2024 NeurIPS AI Hacker Cup](https://hackercupai.github.io/) challenge.\n",
    "\n",
    "#### Goal\n",
    "The goal is to try and solve all 5 of the 2023 practice questions for the AI Hacker Cup using MistralAI's models. We’re offering free MistralAI api access via the code in this colab to get people started.\n",
    "\n",
    "#### Competition GitHub\n",
    "The competition [repo here](https://github.com/tcapelle/hackercup_rag) contains this colab, the code for the Code Generation Agent and the details on how to make a submission and the competition rules.\n",
    "\n",
    "#### Discord\n",
    "You can join the official NeurIPS AI Hacker Cup [discord here](discord.gg/wWeN9hTH32) to share ideas and discuss winning solutions.\n",
    "\n",
    "## Prizes\n",
    "\n",
    "Weights & Biases are giving away a pair of Meta Ray-Ban Smart Glasses for:\n",
    "-  the first individual to submit code that solves 4 out of 5 correct solutions\n",
    "-  the first individual to submit code that solves 5 out of 5 correct solutions\n",
    "\n",
    "## Entry Submissions, Rules & Deadline\n",
    "\n",
    "See the [competition README](https://github.com/tcapelle/hackercup_rag) for how to make a submissions the the competition rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B Weave\n",
    "\n",
    "[W&B Weave](https://weave-docs.wandb.ai/tutorial-eval?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) is used in this competition to run the evaluations. It is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights & Biases. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/weave/master/docs/static/img/evals-hero.png\" width=\"800\" height=\"450\">\n",
    "\n",
    "If you want to learn more about Weave, you can [get started](https://weave-docs.wandb.ai/quickstart?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) by decorating Python functions with `@weave.op`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG for a Code Generation Agent\n",
    "\n",
    "This colab demonstrates how to retrieve over a dataset of coding question-answer pairs (the [CodeContests](https://huggingface.co/datasets/deepmind/code_contests) dataset from DeepMind) in order to find simlar questions that might help our Agent generate the correct solution.\n",
    "\n",
    "A more detailed walkthough of the approach we will use in this notebook can be found in the following **[Youtube video](https://www.youtube.com/watch?v=cObBj2UpWK8)**:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.youtube.com/watch?v=cObBj2UpWK8\">\n",
    "<img src=\"https://img.youtube.com/vi/cObBj2UpWK8/0.jpg\" width=\"400\" height=\"300\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to run this cell only once**\n",
    "We will clone the starter-kits repo\n",
    "Set the rag folder as our working directory\n",
    "and install the dependencies for the project.\n",
    "\n",
    "**You can comment out the cell after you have run it once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starter-kits repo\n",
    "!git clone https://github.com/tcapelle/hackercup_rag\n",
    "# Change directory to the rag folder. Running the next line twice in the same session will raise an error.\n",
    "%cd hackercup_rag\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this colab, create a [free Weights & Biases (W&B) account here](https://wandb.ai/site?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) and then copy your API key from https://wandb.ai/authorize into the input box below when requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weave\n",
    "\n",
    "WEAVE_PROJECT = \"ai-hacker-cup\"\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL for the Mistral model api we'll be using\n",
    "os.environ[\"BASE_URL\"] = \"http://195.242.24.252:8000/v1\"\n",
    "\n",
    "# Select MistralAI models used depending if you want a fast or strong LLM\n",
    "FAST_LLM = \"mistral/open-mistral-nemo-2407\"\n",
    "STRONG_LLM = \"mistral/mistral-large-latest\"\n",
    "os.environ[\"FAST_LLM\"] = FAST_LLM\n",
    "os.environ[\"STRONG_LLM\"] = STRONG_LLM\n",
    "\n",
    "# Set the max tokens for the models and how many parallel requests to make in Weave Evaluations\n",
    "os.environ[\"MAX_TOKENS\"] = \"4096\"\n",
    "os.environ[\"WEAVE_PARALLELISM\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges Dataset\n",
    "We will use the **practice** dataset from the **2023** [HackerCup dataset](https://huggingface.co/datasets/hackercupai/hackercup).\n",
    "\n",
    "We have already processed the dataset and saved it as a [`weave.Dataset`](https://weave-docs.wandb.ai/guides/core-types/datasets/?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup). You can either use the Dataset by running the next cell or download the dataset using the instructions below.\n",
    "\n",
    "We will use this challenge dataset to load some practice problems and solutions from the HackerCup dataset and evaluate our agents on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Problem\n",
    "\n",
    "practice_dataset_uri = \"weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w\"\n",
    "problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]\n",
    "problems = list(map(lambda x: Problem(**x), problems_dataset))\n",
    "problem = problems[0]  # Select the first problem\n",
    "\n",
    "print(\"Sample Problem:\\n\\n\", problem.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Alternative] Download the raw challenges dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can download the dataset by running the download script from the [submit-first-solution](https://github.com/HackerCupAI/starter-kits/tree/main/submit_first_solution). Specifically, you can run the following command to download the dataset:\n",
    "\n",
    "```bash\n",
    "python download.py --year 2023 --dataset_folder data\n",
    "```\n",
    "\n",
    "\n",
    "This should create a `dataset` folder with the problems and solutions. Here's an example of what the data looks like for the `dim_sum_delivery` problem from the `2023` season:\n",
    "\n",
    "```\n",
    "data/dataset/2023/practice\n",
    "...\n",
    "├── dim_sum_delivery.cpp\n",
    "├── dim_sum_delivery.in\n",
    "├── dim_sum_delivery.md\n",
    "├── dim_sum_delivery.out\n",
    "├── dim_sum_delivery_sample_input.txt\n",
    "├── dim_sum_delivery_sample_output.txt\n",
    "├── dim_sum_delivery_sol.md\n",
    "...\n",
    "```\n",
    "\n",
    "Each problem has a `in`, `out`, `md`, `cpp`, and `sol` file.\n",
    "\n",
    "The `in` file contains the input data for the problem.\n",
    "The `out` file contains the expected output for the problem.\n",
    "The `md` file contains the problem statement.\n",
    "The `cpp` file contains the source code to the solution.\n",
    "The `sol` file contains the detailed solution to the problem.\n",
    "The `sample_input.txt` and `sample_output.txt` files contain the sample input and output for the problem. These are the test cases that will be available to the agent during development and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn on logging and asyncio for notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from nest_asyncio import apply\n",
    "\n",
    "apply()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a RAG + Reflection Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent with Reflection\n",
    "\n",
    "We will combine a RAG Agent with Reflection in order to:\n",
    "\n",
    "- Retrieve similar types of questions from the CodeContests dataset, generate a solution, reflect on the solution and test results and improve it.\n",
    "- We then use this improved solution to generate new few-shot examples and repeat the process in a loop until we converge to a solution or the iteration limit is reached.\n",
    "\n",
    "`agent.py` contains the prompts used for analysis (`ANALYSIS_INSTRUCTIONS`), reflection (`REFLECTION_INSTRUCTIONS`) and problem solving (`SOLVER_INSTRUCTIONS`) feel free to edit them to improve the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import REFLECTION_INSTRUCTIONS\n",
    "\n",
    "print(REFLECTION_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "\n",
    "The code used the retrieval over the CodeContests dataset can be found in `retriever.py`. Here we'll initialise our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import Retriever\n",
    "\n",
    "retriever = Retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Solver Pipeline\n",
    "\n",
    "Here we run the code generation pipeline which:\n",
    "- given a problem, retrieves similar problems from the CodeCompletions dataset\n",
    "- generates candidate code for problem\n",
    "- executes the code\n",
    "- checks if the executed code generates the correct solution\n",
    "- if the solution is correct, it terminates otherwise it retries for `max_iterations`\n",
    "\n",
    "Note `code_execution_timeout`is used to limit the time available for the generated python code to execute as sometimes the code generated be recursive code that never terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import rag_solver\n",
    "from agent import rework_solution\n",
    "\n",
    "@weave.op\n",
    "async def rag_solver_with_reflection(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.7,\n",
    "        max_iterations: int = 2,\n",
    "        code_execution_timeout: int = 10,\n",
    "):\n",
    "    num_iterations = 0\n",
    "    test_report = \"failed\"\n",
    "    solution = None\n",
    "    while not test_report == \"passed\" and num_iterations < max_iterations:\n",
    "        rag_result = await rag_solver(\n",
    "            retriever=retriever,\n",
    "            problem=problem,\n",
    "            timeout=code_execution_timeout,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        solution = rag_result[\"solution\"]\n",
    "        test_report = rag_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            logger.info(f\"Passing solution generated successfully for problem: {problem.problem_name}\")\n",
    "            return rag_result\n",
    "        \n",
    "        logger.info(f\"Solution failed, reworking solution. Problem: {problem.problem_name}\")\n",
    "        rework_result = await rework_solution(\n",
    "            problem=problem,\n",
    "            incorrect_solution=solution,\n",
    "            test_report=test_report,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            timeout=code_execution_timeout,\n",
    "        )\n",
    "        solution = rework_result[\"solution\"]\n",
    "        test_report = rework_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            logger.info(f\"Re-worked solution passed for problem: {problem.problem_name}\")\n",
    "            return {\n",
    "                \"solution\": solution,\n",
    "                \"stage\": \"reflection\",\n",
    "                \"test_report\": test_report,\n",
    "            }\n",
    "        num_iterations += 1\n",
    "        logger.info(f\"Re-worked solution failed, trying iteration {num_iterations}. Problem: {problem.problem_name}\")\n",
    "    logger.info(\"Failed to generate a solution after {num_iterations} iterations. Problem: {problem.problem_name}\")\n",
    "    return {\"solution\": solution, \"stage\": \"failed\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the pipeline on 1 problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_result = await rag_solver_with_reflection(\n",
    "    retriever, problem, max_iterations=2, code_execution_timeout=30\n",
    ")\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate against the expected solutions.\n",
    "\n",
    "### Create a Weave Model\n",
    "First we create a Weave [\"Model\"](https://weave-docs.wandb.ai/guides/core-types/models?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup), which has a `predict` function that Weave Evaluations will call to generate a solution. It also has various attributes that we can set to adjust the behaviour of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGReflectionAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    max_iterations: int = 2\n",
    "    code_execution_timeout: int = 30\n",
    "    model: str = STRONG_LLM\n",
    "    temperature: float = 0.7\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver_with_reflection(\n",
    "            self.retriever,\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_iterations=self.max_iterations,\n",
    "            code_execution_timeout=self.code_execution_timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Scorer and define the Evals Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the output of the \"test_report\" from our solver above to be `\"passed\"` if the solution is correct. You can think of `expected_result` in the `evals_dataset` as the label that the `test_report` from our solver needs to return in order to ensure the generated solution is correct.\n",
    "\n",
    "Weave Evaluations expect a list of dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dataset = [{\"problem\": problem, \"expected_result\": \"passed\"} for problem in problems]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weave Evaluations use a scorer function that returns a metric and its result in a dict. Here we define a metric that checks if the code generated by agent passed the test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def scorer(expected_result: str, model_output: dict) -> dict:\n",
    "    if model_output is None or model_output[\"test_report\"] is None:\n",
    "        return {\"solution_passed\": False}\n",
    "    return {\"solution_passed\": expected_result == model_output[\"test_report\"]}\n",
    "\n",
    "\n",
    "# Weave Evaluations take a dataset and scoring functions.\n",
    "# This is a evaluation that checks if the code generated by agent passed the test\n",
    "evaluator = weave.Evaluation(dataset=evals_dataset, scorers=[scorer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    "Now we instantiate the Agent and run the evaluation. Results from the evaluation will be printed in the W&B Weave UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG reflection agent\n",
    "tasks = []\n",
    "\n",
    "LLM = STRONG_LLM\n",
    "eval_temperature = 0.7\n",
    "\n",
    "# Instantiate the agent, which is a subclass of `weave.Model`\n",
    "rag_reflection_agent = RAGReflectionAgent(\n",
    "    retriever=retriever, model=LLM, temperature=eval_temperature, code_execution_timeout=30\n",
    ")\n",
    "\n",
    "# Evaluate the agent by passing it to the evaluator\n",
    "# Weave Evaluations are async, so we use `asyncio.gather` to run them in parallel\n",
    "rag_reflection_results = evaluator.evaluate(rag_reflection_agent)\n",
    "tasks.append(rag_reflection_results)\n",
    "\n",
    "rag_reflection_results = await asyncio.gather(*tasks)\n",
    "\n",
    "logger.info(rag_reflection_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
