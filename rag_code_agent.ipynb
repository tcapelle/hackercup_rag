{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcapelle/hackercup_rag/blob/main/rag_code_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{rag-hackercup} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "# W&B Lighting Competition - AI Hacker Cup \n",
    "\n",
    "</a>\n",
    "\n",
    "[Weights & Biases](https://wandb.ai/site?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) are running a 7-day Lightning Competition focussed on solving practice problems for the  [2024 NeurIPS AI Hacker Cup](https://hackercupai.github.io/) challenge.\n",
    "\n",
    "#### Goal\n",
    "The goal is to try and solve all 5 of the 2023 practice questions for the AI Hacker Cup using MistralAI's models. We’re offering free MistralAI api access via the code in this colab to get people started.\n",
    "\n",
    "#### Competition GitHub\n",
    "The competition [repo here](https://github.com/tcapelle/hackercup_rag) contains this colab, the code for the Code Generation Agent and the details on how to make a submission and the competition rules.\n",
    "\n",
    "#### Discord\n",
    "You can join the official NeurIPS AI Hacker Cup [discord here](discord.gg/wWeN9hTH32) to share ideas and discuss winning solutions.\n",
    "\n",
    "## Prizes\n",
    "\n",
    "Weights & Biases are giving away a pair of Meta Ray-Ban Smart Glasses for:\n",
    "-  the first individual to submit code that solves 4 out of 5 correct solutions\n",
    "-  the first individual to submit code that solves 5 out of 5 correct solutions\n",
    "\n",
    "## Entry Submissions, Rules & Deadline\n",
    "\n",
    "See the [competition README](https://github.com/tcapelle/hackercup_rag) for how to make a submissions the the competition rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B Weave\n",
    "\n",
    "[W&B Weave](https://weave-docs.wandb.ai/tutorial-eval?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) is used in this competition to run the evaluations. It is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights & Biases. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/weave/master/docs/static/img/evals-hero.png\" width=\"800\" height=\"450\">\n",
    "\n",
    "If you want to learn more about Weave, you can [get started](https://weave-docs.wandb.ai/quickstart?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) by decorating Python functions with `@weave.op`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG for a Code Generation Agent\n",
    "\n",
    "This colab demonstrates how to retrieve over a dataset of coding question-answer pairs (the [CodeContests](https://huggingface.co/datasets/deepmind/code_contests) dataset from DeepMind) in order to find simlar questions that might help our Agent generate the correct solution.\n",
    "\n",
    "A more detailed walkthough of the approach we will use in this notebook can be found in the following **[Youtube video](https://www.youtube.com/watch?v=cObBj2UpWK8)**:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.youtube.com/watch?v=cObBj2UpWK8\">\n",
    "<img src=\"https://img.youtube.com/vi/cObBj2UpWK8/0.jpg\" width=\"400\" height=\"300\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to run this cell only once**\n",
    "We will clone the starter-kits repo\n",
    "Set the rag folder as our working directory\n",
    "and install the dependencies for the project.\n",
    "\n",
    "**You can comment out the cell after you have run it once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starter-kits repo\n",
    "!git clone https://github.com/tcapelle/hackercup_rag\n",
    "# Change directory to the rag folder. Running the next line twice in the same session will raise an error.\n",
    "%cd hackercup_rag\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this colab, create a [free Weights & Biases (W&B) account here](https://wandb.ai/site?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) and then copy your API key from https://wandb.ai/authorize into the input box below when requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: capecape.\n",
      "View Weave data at https://wandb.ai/capecape/ai-hacker-cup/weave\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weave\n",
    "\n",
    "WEAVE_PROJECT = \"ai-hacker-cup\"\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL for the Mistral model api we'll be using\n",
    "os.environ[\"BASE_URL\"] = \"http://195.242.24.252:8000/v1\"\n",
    "\n",
    "# Select MistralAI models used depending if you want a fast or strong LLM\n",
    "FAST_LLM = \"open-mistral-nemo-2407\"\n",
    "STRONG_LLM = \"mistral-large-latest\"\n",
    "os.environ[\"FAST_LLM\"] = FAST_LLM\n",
    "os.environ[\"STRONG_LLM\"] = STRONG_LLM\n",
    "\n",
    "# Set the max tokens for the models and how many parallel requests to make in Weave Evaluations\n",
    "os.environ[\"MAX_TOKENS\"] = \"4096\"\n",
    "os.environ[\"WEAVE_PARALLELISM\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges Dataset\n",
    "We will use the **practice** dataset from the **2023** [HackerCup dataset](https://huggingface.co/datasets/hackercupai/hackercup).\n",
    "\n",
    "We have already processed the dataset and saved it as a [`weave.Dataset`](https://weave-docs.wandb.ai/guides/core-types/datasets/?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup). You can either use the Dataset by running the next cell or download the dataset using the instructions below.\n",
    "\n",
    "We will use this challenge dataset to load some practice problems and solutions from the HackerCup dataset and evaluate our agents on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 17:18:45,768 : INFO : Use pytorch device_name: mps\n",
      "2024-09-06 17:18:45,768 : INFO : Load pretrained SentenceTransformer: jinaai/jina-embeddings-v2-base-code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Problem:\n",
      "\n",
      " {\n",
      "  \"problem_dir\": \"data/2023/practice\",\n",
      "  \"problem_name\": \"two_apples_a_day\",\n",
      "  \"problem_description\": \"“An apple a day keeps the doctor away” is Steve’s motto. His other motto, “You can never have too much of a good thing,” holds true for both apples and mottos. Steve would like to eat two apples per day for the next \\\\(N\\\\) days, but with strict adherence to his third motto “Consistency is key.” Specifically, he’d like the sum of the two apple weights he eats over the next \\\\(N\\\\) days to be the same for each day.\\n\\nSteve has already purchased \\\\(2*N-1\\\\) apples, the \\\\(i\\\\)th of which weighs \\\\(A_i\\\\) ounces. He'd like to buy one more apple that's as light as possible to fulfill his goal. Steve can buy an apple of any positive integer weight in ounces from the store. Is it possible for him to reach his goal, and if so, what weight apple should he buy?\\n\\n{{PHOTO_ID:1563872647765708|WIDTH:600}}\\n\\n\\n*The above image depicts the solution to the first sample. Each day, Steve will eat two apples totalling \\\\(7\\\\) oz. Steve must buy a \\\\(4\\\\) oz apple to make this happen.*\\n\\n# Constraints\\n\\\\(1 \\\\leq T \\\\leq 70\\\\)\\n\\\\(1 \\\\leq N \\\\leq 3*10^5\\\\)\\nThe sum of \\\\(N\\\\) over all cases is at most \\\\(600{,}000\\\\)\\n\\\\(1 \\\\leq A_i \\\\leq  10^9\\\\)\\n\\n# Input Format\\nInput begins with an integer \\\\(T\\\\), the number of test cases. Each test case starts with a single integer \\\\(N\\\\). The next line contains \\\\(2*N-1\\\\) space-separated integers \\\\(A_1, ..., A_{2*N - 1}\\\\).\\n\\n# Output Format\\nFor the \\\\(i\\\\)th test case, print \\\"`Case #i:` \\\" followed a single integer, the smallest possible apple weight in ounces that Steve can buy so that he can eat two apples for the next \\\\(N\\\\) days and have the sum of apple weights be the same every day, or \\\\(-1\\\\) if doing so is impossible.\\n\\n# Sample Explanation\\n\\nIn the first case, if Steve buys a \\\\(4\\\\) oz apple, he can group his apples as shown above. For this input, there's no way to succeed by buying any apple below \\\\(4\\\\) oz.\\n\\nIn the second case, Steve can buy a \\\\(7\\\\) oz apple, and eat two apples totaling \\\\(14\\\\) oz each day.\\n\\nIn the third case, any apple weight will suffice, so Steve will buy the lightest one possible.\\n\\nIn the fourth case, no matter what weight apple Steve attempts to buy, it is impossible for him to achieve his goal.\\n\\nPlease note, as demonstrated in the seventh case, that it's possible for the answer to exceed \\\\(10^9\\\\).\\n\\n\\n\",\n",
      "  \"sample_input\": \"7\\n3\\n6 3 1 2 5\\n2\\n7 7 7\\n1\\n1\\n3\\n1 9 1 1 4\\n4\\n1 9 1 1 4 9 9\\n4\\n1 9 10 1 4 6 9\\n3\\n1000000000 2 10 4 999999994\\n\",\n",
      "  \"sample_output\": \"Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n\",\n",
      "  \"problem_input\": \"data/2023/practice/two_apples_a_day.in\",\n",
      "  \"problem_output\": \"data/2023/practice/two_apples_a_day.out\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from agent import rag_solver, rework_solution\n",
    "from utils import Problem\n",
    "\n",
    "practice_dataset_uri = \"weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w\"\n",
    "problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]\n",
    "problems = list(map(lambda x: Problem(**x), problems_dataset))\n",
    "problem = problems[0]  # Select the first problem\n",
    "\n",
    "print(\"Sample Problem:\\n\\n\", problem.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Alternative] Download the raw challenges dataset\n",
    "\n",
    "You can alternatively download the full raw challenges dataset, see the README to see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn on logging and asyncio for notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from nest_asyncio import apply\n",
    "\n",
    "apply()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a RAG + Reflection Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent with Reflection\n",
    "\n",
    "We will combine a RAG Agent with Reflection in order to:\n",
    "\n",
    "- Retrieve similar types of questions from the CodeContests dataset, generate a solution, reflect on the solution and test results and improve it.\n",
    "- We then use this improved solution to generate new few-shot examples and repeat the process in a loop until we converge to a solution or the iteration limit is reached.\n",
    "\n",
    "`agent.py` contains the prompts used for analysis (`ANALYSIS_INSTRUCTIONS`), reflection (`REFLECTION_INSTRUCTIONS`) and problem solving (`SOLVER_INSTRUCTIONS`) feel free to edit them to improve the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a world-class competitive programmer with a keen eye for detail and problem solving. \n",
      "Your expertise is in algorithms and data structures. \n",
      "You have incorrectly answered the following programming problem. \n",
      "Your task is to reflect on the problem, your solution, and the correct answer.\n",
      "You will then use this information help you answer the same question in the future. \n",
      "First, explain why you answered the question incorrectly.\n",
      "Second, list the keywords that describe the type of your errors from most general to most specific.\n",
      "Third, solve the problem again, step-by-step, based on your knowledge of the correct answer.\n",
      "Fourth, create a list of detailed instructions to help you correctly solve this problem in the future.\n",
      "Finally, create a list of general advice to help you solve similar types of problems in the future.\n",
      "Be concise in your response; however, capture all of the essential information.\n",
      "\n",
      "{problem}\n",
      "<incorrect_solution>\n",
      "{incorrect_solution}\n",
      "</incorrect_solution>\n",
      "<test_report>\n",
      "{test_report}\n",
      "</test_report>\n",
      "\n",
      "**Format Instructions: Your response must follow the following xml format** -\n",
      "\n",
      "<root>\n",
      "<reflection>\n",
      "[Reflect on the problem, your solution, and the correct answer.]\n",
      "</reflection>\n",
      "<keywords>\n",
      "[List the keywords that describe the type of your errors from most general to most specific.]\n",
      "</keywords>\n",
      "<step_by_step_solution>\n",
      "[Solve the problem again, step-by-step, based on your knowledge of the correct answer.]\n",
      "</step_by_step_solution>\n",
      "<instructions>\n",
      "[Create a list of detailed instructions to help you correctly solve this problem in the future.]\n",
      "</instructions>\n",
      "<general_advice>\n",
      "[Create a list of general advice to help you solve similar types of problems in the future.]\n",
      "</general_advice>\n",
      "</root>\n",
      "---\n",
      "Let's think step by step to reflect on the problem:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agent import REFLECTION_INSTRUCTIONS\n",
    "\n",
    "print(REFLECTION_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "\n",
    "The code used the retrieval over the CodeContests dataset can be found in `retriever.py`. Here we'll initialise our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 17:18:55,859 : DEBUG : Building index from IDs objects      \n",
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "from retriever import Retriever\n",
    "\n",
    "retriever = Retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Solver Pipeline\n",
    "\n",
    "Here we run the code generation pipeline which:\n",
    "- given a problem, retrieves similar problems from the CodeCompletions dataset\n",
    "- generates candidate code for problem\n",
    "- executes the code\n",
    "- checks if the executed code generates the correct solution\n",
    "- if the solution is correct, it terminates otherwise it retries for `max_iterations`\n",
    "\n",
    "Note `code_execution_timeout`is used to limit the time available for the generated python code to execute as sometimes the code generated be recursive code that never terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def rag_solver_with_reflection(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.7,\n",
    "        max_iterations: int = 2,\n",
    "        code_execution_timeout: int = 10,\n",
    "):\n",
    "    num_iterations = 0\n",
    "    test_report = \"failed\"\n",
    "    solution = None\n",
    "    while not test_report == \"passed\" and num_iterations < max_iterations:\n",
    "        rag_result = await rag_solver(\n",
    "            retriever=retriever,\n",
    "            problem=problem,\n",
    "            timeout=code_execution_timeout,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        solution = rag_result[\"solution\"]\n",
    "        test_report = rag_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            logger.info(f\"Passing solution generated successfully for problem: {problem.problem_name}\")\n",
    "            return rag_result\n",
    "        \n",
    "        logger.info(f\"Solution failed, reworking solution. Problem: {problem.problem_name}\")\n",
    "        rework_result = await rework_solution(\n",
    "            problem=problem,\n",
    "            incorrect_solution=solution,\n",
    "            test_report=test_report,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            timeout=code_execution_timeout,\n",
    "        )\n",
    "        solution = rework_result[\"solution\"]\n",
    "        test_report = rework_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            logger.info(f\"Re-worked solution passed for problem: {problem.problem_name}\")\n",
    "            return {\n",
    "                \"solution\": solution,\n",
    "                \"stage\": \"reflection\",\n",
    "                \"test_report\": test_report,\n",
    "            }\n",
    "        num_iterations += 1\n",
    "        logger.info(f\"Re-worked solution failed, trying iteration {num_iterations}. Problem: {problem.problem_name}\")\n",
    "    logger.info(\"Failed to generate a solution after {num_iterations} iterations. Problem: {problem.problem_name}\")\n",
    "    return {\"solution\": solution, \"stage\": \"failed\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the pipeline on 1 problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 17:19:04,144 : INFO : Drafting intial zero-shot solution\n",
      "2024-09-06 17:19:25,815 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:19:33,982 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:19:37,661 : INFO : Draft solution result: \"WRONG ANSWER!!\\n\\n<expected>\\n'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n'\\n</expected>\\n---\\n<got>\\n'Case #1: -1\\nCase #2: -1\\nCase #3: -1\\nCase #4: -1\\nCase #5: 5\\nCase #6: 4\\nCase #7: -1\\n'\\n</got>\"\n",
      "2024-09-06 17:19:37,662 : INFO : Iterating on a RAG solution\n",
      "2024-09-06 17:19:38,306 : INFO : Generating RAG solution:\n",
      "2024-09-06 17:19:38,937 : INFO : Generating examplars:\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:08<00:00,  2.98it/s]\n",
      "2024-09-06 17:19:59,399 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:00,749 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:02,205 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:02,206 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:06,828 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:10,359 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:13,167 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:15,864 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:15,866 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:20:24,351 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:21:13,066 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:21:26,054 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-06 17:21:28,354 : INFO : RAG Solution Result: 'failed:   File \"<string>\", line 34\\n    sys.stdout.write(\"\\n                     ^\\nSyntaxError: unterminated string literal (detected at line 34)\\n'\n",
      "2024-09-06 17:21:28,355 : INFO : Solution failed, reworking solution. Problem: two_apples_a_day\n",
      "2024-09-06 17:21:29,013 : INFO : Reflecting and improving solution\n",
      "2024-09-06 17:21:44,216 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:21:50,440 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:22:24,465 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:22:35,973 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-06 17:22:38,513 : INFO : Reworked solution result: 'failed:   File \"<string>\", line 28\\n    sys.stdout.write(\"\\n                     ^\\nSyntaxError: unterminated string literal (detected at line 28)\\n'\n",
      "2024-09-06 17:22:38,514 : INFO : Re-worked solution failed, trying iteration 1. Problem: two_apples_a_day\n",
      "2024-09-06 17:22:38,515 : INFO : Drafting intial zero-shot solution\n",
      "2024-09-06 17:23:02,547 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:12,585 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-06 17:23:15,552 : INFO : Draft solution result: \"WRONG ANSWER!!\\n\\n<expected>\\n'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n'\\n</expected>\\n---\\n<got>\\n'Case #1: 2\\nCase #2: -1\\nCase #3: -1\\nCase #4: -1\\nCase #5: -1\\nCase #6: -1\\nCase #7: -1\\n'\\n</got>\"\n",
      "2024-09-06 17:23:15,553 : INFO : Iterating on a RAG solution\n",
      "2024-09-06 17:23:16,245 : INFO : Generating RAG solution:\n",
      "2024-09-06 17:23:16,898 : INFO : Generating examplars:\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:06<00:00,  3.59it/s]\n",
      "2024-09-06 17:23:30,707 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:32,118 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:33,615 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:35,022 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:35,023 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:38,932 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:41,300 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:46,531 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:48,828 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:23:48,830 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:24:30,379 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:24:41,319 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-06 17:24:43,723 : INFO : RAG Solution Result: 'failed:   File \"<string>\", line 11\\n    weights = list(map(int, data[index + 1:index + 2 * N]))\\nIndentationError: unexpected indent\\n'\n",
      "2024-09-06 17:24:43,724 : INFO : Solution failed, reworking solution. Problem: two_apples_a_day\n",
      "2024-09-06 17:24:43,725 : INFO : Reflecting and improving solution\n",
      "2024-09-06 17:25:04,555 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:25:11,501 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:25:40,469 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-06 17:25:50,701 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-06 17:25:53,077 : INFO : Reworked solution result: 'failed:   File \"<string>\", line 25\\n    sys.stdout.write(\"\\n                     ^\\nSyntaxError: unterminated string literal (detected at line 25)\\n'\n",
      "2024-09-06 17:25:53,078 : INFO : Re-worked solution failed, trying iteration 2. Problem: two_apples_a_day\n",
      "2024-09-06 17:25:53,078 : INFO : Failed to generate a solution after {num_iterations} iterations. Problem: {problem.problem_name}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "import sys\n",
      "\n",
      "def main():    input = sys.stdin.read\n",
      "data = input().split()\n",
      "\n",
      "T = int(data[0])\n",
      "index = 1\n",
      "results = []\n",
      "\n",
      "for _ in range(T):\n",
      "    N = int(data[index])\n",
      "    weights = list(map(int, data[index + 1:index + 2 * N]))\n",
      "    index += 2 * N\n",
      "\n",
      "    total_sum = sum(weights)\n",
      "    target_sum = total_sum // N\n",
      "\n",
      "    if total_sum % N == 0:\n",
      "        additional_weight = target_sum - weights[2 * N - 1]\n",
      "    else:\n",
      "        additional_weight = -1\n",
      "\n",
      "    results.append(f\"Case #{_ + 1}: {additional_weight}\")\n",
      "\n",
      "sys.stdout.write(\"\\n\".join(results) + \"\\n\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "********************************************************************************\n",
      "failed:   File \"<string>\", line 25\n",
      "    sys.stdout.write(\"\n",
      "                     ^\n",
      "SyntaxError: unterminated string literal (detected at line 25)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reflection_result = await rag_solver_with_reflection(\n",
    "    retriever, problem, STRONG_LLM, max_iterations=2, code_execution_timeout=30\n",
    ")\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate against the expected solutions.\n",
    "\n",
    "### Create a Weave Model\n",
    "First we create a Weave [\"Model\"](https://weave-docs.wandb.ai/guides/core-types/models?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup), which has a `predict` function that Weave Evaluations will call to generate a solution. It also has various attributes that we can set to adjust the behaviour of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGReflectionAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    max_iterations: int = 2\n",
    "    code_execution_timeout: int = 30\n",
    "    model: str = STRONG_LLM\n",
    "    temperature: float = 0.7\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver_with_reflection(\n",
    "            self.retriever,\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_iterations=self.max_iterations,\n",
    "            code_execution_timeout=self.code_execution_timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Scorer and define the Evals Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the output of the \"test_report\" from our solver above to be `\"passed\"` if the solution is correct. You can think of `expected_result` in the `evals_dataset` as the label that the `test_report` from our solver needs to return in order to ensure the generated solution is correct.\n",
    "\n",
    "Weave Evaluations expect a list of dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dataset = [{\"problem\": problem, \"expected_result\": \"passed\"} for problem in problems]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weave Evaluations use a scorer function that returns a metric and its result in a dict. Here we define a metric that checks if the code generated by agent passed the test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def scorer(expected_result: str, model_output: dict) -> dict:\n",
    "    if model_output is None or model_output[\"test_report\"] is None:\n",
    "        return {\"solution_passed\": False}\n",
    "    return {\"solution_passed\": expected_result == model_output[\"test_report\"]}\n",
    "\n",
    "\n",
    "# Weave Evaluations take a dataset and scoring functions.\n",
    "# This is a evaluation that checks if the code generated by agent passed the test\n",
    "evaluator = weave.Evaluation(dataset=evals_dataset, scorers=[scorer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Evaluation\n",
    "Now we instantiate the Agent and run the evaluation. Results from the evaluation will be printed in the W&B Weave UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RAG reflection agent\n",
    "tasks = []\n",
    "\n",
    "LLM = STRONG_LLM\n",
    "eval_temperature = 0.7\n",
    "\n",
    "# Instantiate the agent, which is a subclass of `weave.Model`\n",
    "rag_reflection_agent = RAGReflectionAgent(\n",
    "    retriever=retriever, model=LLM, temperature=eval_temperature, code_execution_timeout=30\n",
    ")\n",
    "\n",
    "# Evaluate the agent by passing it to the evaluator\n",
    "# Weave Evaluations are async, so we use `asyncio.gather` to run them in parallel\n",
    "rag_reflection_results = evaluator.evaluate(rag_reflection_agent)\n",
    "tasks.append(rag_reflection_results)\n",
    "\n",
    "rag_reflection_results = await asyncio.gather(*tasks)\n",
    "\n",
    "logger.info(rag_reflection_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
